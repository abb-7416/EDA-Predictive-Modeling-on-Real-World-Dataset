# ğŸ“Š Telco Customer Churn â€” EDA & Predictive Modeling

## ğŸ¯ Objective
Perform **Exploratory Data Analysis (EDA)** and build **Machine Learning predictive models** to classify customers likely to churn using the **Telco Customer Churn** dataset.

---

## ğŸ“¦ Dataset
**Source:** [Telco Customer Churn Dataset on Kaggle](https://www.kaggle.com/datasets/blastchar/telco-customer-churn)  

This dataset contains customer information for a telecom company â€” including demographic details, account information, and service usage â€” with a binary target variable **`Churn`** indicating whether the customer left the company.

**File:** `WA_Fn-UseC_-Telco-Customer-Churn.csv`

### Key Features
| Column | Description |
|:--------|:-------------|
| `gender`, `SeniorCitizen`, `Partner`, `Dependents` | Customer demographics |
| `tenure` | Number of months customer has stayed |
| `PhoneService`, `InternetService`, `OnlineSecurity`, ... | Services subscribed |
| `Contract`, `PaymentMethod`, `PaperlessBilling` | Account/contract details |
| `MonthlyCharges`, `TotalCharges` | Billing information |
| `Churn` | Target variable (`Yes` / `No`) |

---

## ğŸ§° Tools and Libraries
| Category | Tools Used |
|:----------|:------------|
| **Language** | Python (3.8+) |
| **Environment** | Jupyter Notebook |
| **Libraries** | `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`, `joblib`, `imbalanced-learn` (for SMOTE) |

---

## ğŸ§± Step-by-Step Process

### 1ï¸âƒ£ Data Loading and Inspection
- Imported the dataset into a Pandas DataFrame.
- Verified dataset size, structure, and column data types.
- Checked for duplicates and missing values.
- Converted `TotalCharges` from `object` to numeric and handled blank spaces.

---

### 2ï¸âƒ£ Data Cleaning
- Filled missing `TotalCharges` values with median or 0.
- Removed irrelevant columns such as `customerID`.
- Ensured data consistency in categorical fields (`Yes`/`No` values).

---

### 3ï¸âƒ£ Exploratory Data Analysis (EDA)
Performed both **univariate** and **bivariate** analysis:

#### ğŸ”¹ Univariate Analysis
- Visualized distributions of numeric variables (`tenure`, `MonthlyCharges`, `TotalCharges`) using histograms.
- Analyzed churn proportion (`Yes` vs `No`).

#### ğŸ”¹ Bivariate Analysis
- Compared churn rate across categories (e.g., `Contract`, `PaymentMethod`, `InternetService`).
- Boxplots for numeric variables grouped by churn.
- Heatmap of correlations among numeric variables.

#### ğŸ”¹ Insights Example
- Month-to-month contracts have higher churn.
- Longer tenure customers are less likely to churn.
- Higher `MonthlyCharges` slightly increases churn risk.

---

### 4ï¸âƒ£ Feature Engineering
- Created `tenure_group` bins (e.g., `0â€“12`, `13â€“24`, â€¦).
- Mapped `SeniorCitizen` 0/1 â†’ `No`/`Yes`.
- Encoded categorical variables using **OneHotEncoder**.
- Scaled numerical features (`tenure`, `MonthlyCharges`, `TotalCharges`) with **StandardScaler** for Logistic Regression.

---

### 5ï¸âƒ£ Handling Class Imbalance
- Checked class imbalance: churn rate typically â‰ˆ 26%.
- Applied **SMOTE (Synthetic Minority Over-sampling Technique)** using `imblearn.over_sampling.SMOTE` to balance the training data.
- Alternatively, used `class_weight='balanced'` in models (for Logistic Regression and Random Forest).

---

### 6ï¸âƒ£ Model Building

#### ğŸ“˜ Model 1: Logistic Regression
- Used pipeline: preprocessing (scaling + encoding) â†’ `LogisticRegression(class_weight='balanced')`.
- Tuned hyperparameters using **GridSearchCV** for `C` (regularization strength).
- Model emphasizes interpretability and linear decision boundaries.

#### ğŸŒ² Model 2: Random Forest
- Used pipeline: preprocessing â†’ `RandomForestClassifier(class_weight='balanced')`.
- Tuned parameters using **GridSearchCV** for:
  - `n_estimators` (100â€“300)
  - `max_depth` (5â€“20)
  - `min_samples_split`, `min_samples_leaf`
- Captures nonlinear relationships and feature interactions.

---

### 7ï¸âƒ£ Model Evaluation Metrics
Evaluated models using **test data** on multiple metrics:

| Metric | Description |
|:--------|:-------------|
| **Accuracy** | Overall correct predictions |
| **Precision** | Fraction of true churns among predicted churns |
| **Recall (Sensitivity)** | Fraction of actual churns correctly identified |
| **F1 Score** | Harmonic mean of precision and recall |
| **ROC AUC** | Area under ROC curve â€” modelâ€™s ability to distinguish classes |
| **Confusion Matrix** | True vs predicted outcomes visualization |

#### Visual Evaluations
- ROC Curve comparison (Logistic Regression vs Random Forest)
- Feature importance visualization from Random Forest

---

### 8ï¸âƒ£ Model Saving
- Best-performing model (based on ROC AUC) saved using:
  ```python
  joblib.dump(best_model, 'best_churn_model.joblib')
  
### Key Observations
Churn rate: â‰ˆ 26%
Top features influencing churn:
Contract type (Month-to-month customers churn more)
Tenure (shorter tenure = higher churn)
MonthlyCharges (higher bills correlate with churn)
InternetService type (Fiber optic customers churn more)
Model performance:
Logistic Regression AUC â‰ˆ 0.83
Random Forest AUC â‰ˆ 0.87
Random Forest chosen as final model (better generalization and feature insight)

**ğŸš€ How to Run the Project**
Install dependencies
pip install pandas numpy matplotlib seaborn scikit-learn imbalanced-learn joblib
Download dataset
From Kaggle Telco Customer Churn
Place the CSV file in the same directory as the notebook.
Run notebook
Open Jupyter Notebook.
Execute all cells in eda_predictive_model.ipynb.
Observe outputs
Plots: distribution, correlation, churn patterns.
Metrics: accuracy, F1 score, ROC AUC.
Saved model file in project folder.

**ğŸ“š Learning Outcome**
By completing this project, youâ€™ll gain hands-on experience in:
Data cleaning and preprocessing real-world data
Visualizing data distributions and relationships (EDA).
Handling categorical and numerical features in pipelines.
Managing imbalanced datasets using SMOTE or class weights.
Building and evaluating supervised ML models.
Comparing model performance using ROC AUC and F1 score.
Saving and reusing trained models for deployment.

**ğŸ Summary**
This project demonstrates an end-to-end Data Science workflow:
Data â†’ EDA â†’ Feature Engineering â†’ Model Building â†’ Evaluation â†’ Deployment-ready Model
It combines statistical understanding, Python programming, and machine learning skills, preparing you for practical analytics and AI-driven business problem solving.
